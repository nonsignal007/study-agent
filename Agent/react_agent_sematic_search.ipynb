{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add semantic search to your agent's memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"qwen2.5:14b\",\n",
    "    base_url='http://127.0.0.1:11434/v1',\n",
    "    api_key='ollama',\n",
    "    temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Store 생성\n",
    "\n",
    "- Semantic 검색이 가능한 메모리 저장소 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "embedings = init_embeddings('huggingface:BAAI/bge-m3')\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        'embed': embedings,\n",
    "        'dims' : 1024\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store some memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.put(('user_123','memories'),'1',{'text': 'I love pizza'})\n",
    "store.put(('user_123','memories'),'2',{'text': 'I prefer Italian food'})\n",
    "store.put(('user_123','memories'),'3',{'text': \"I don't like spicy food\"})\n",
    "store.put(('user_123','memories'),'3',{'text': 'I am studying econometrics'})\n",
    "store.put(('user_123','memories'),'3',{'text': 'I am a plumber'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search memories using natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: I prefer Italian food (similarity: 0.6429529122454709)\n",
      "Memory: I am a plumber (similarity: 0.6412285135022295)\n",
      "Memory: I love pizza (similarity: 0.6045187378022228)\n"
     ]
    }
   ],
   "source": [
    "memories = store.search(('user_123', 'memories'), query=\"I'm hungry\", limit=3)\n",
    "\n",
    "for memory in memories:\n",
    "    print(f\"Memory: {memory.value['text']} (similarity: {memory.score})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using in your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since you mentioned that you prefer Italian food, how about trying some pasta or pizza for lunch? If you're looking for something quick and easy, maybe a margherita pizza or spaghetti with marinara sauce would be perfect. Do you have any favorite spots nearby, or should I suggest some places to order from?"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "llm = model\n",
    "\n",
    "def chat(state, *, store: BaseStore):\n",
    "    # Search based on user's last message\n",
    "\n",
    "    items = store.search(\n",
    "        ('user_123', 'memories'), query=state['messages'][-1].content, limit=2\n",
    "    )\n",
    "\n",
    "    memories = '\\n'.join(item.value['text'] for item in items)\n",
    "    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            {'role': 'system' , 'content': f\"You are a helpful assistant.\\n{memories}\"},\n",
    "            *state['messages'],\n",
    "        ]\n",
    "    )\n",
    "    return {'messages' : [response]}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(chat)\n",
    "builder.add_edge(START, 'chat')\n",
    "graph = builder.compile(store=store)\n",
    "\n",
    "for message, metadata in graph.stream(\n",
    "    input = {'messages' : [{'role': 'user' , 'content' : \"I'm hungry\"}]},\n",
    "    stream_mode='messages'\n",
    "):\n",
    "    print(message.content, end = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using in `create_react_agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Optional\n",
    "\n",
    "from langgraph.prebuilt import InjectedStore\n",
    "from langgraph.store.base import BaseStore\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def prepare_messages(state, *, store: BaseStore):\n",
    "    items = store.search(\n",
    "        (\"user_123\", \"memories\"), query=state['messages'][-1].content, limit=2\n",
    "    )\n",
    "\n",
    "    memories = \"\\n\".join(item.value['text'] for item in items)\n",
    "    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\" : f\"You are a helpful assistant.\\n{memories}\"}\n",
    "    ] + state['messages']\n",
    "\n",
    "def upsert_memory(\n",
    "        content: str,\n",
    "        *,\n",
    "        memory_id: Optional[uuid.UUID] = None,\n",
    "        store: Annotated[BaseStore, InjectedStore],\n",
    "):\n",
    "    \"\"\"Upsert a memory in the database\"\"\"\n",
    "    # The LLM can use this tool to store a new memory\n",
    "    mem_id = memory_id or uuid.uuid4()\n",
    "    store.put(\n",
    "        (\"user_123\", \"memories\"),\n",
    "        key = str(mem_id),\n",
    "        value = {'text': content}\n",
    "    )\n",
    "    return f\"Stroed memory {mem_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(\n",
    "    model,\n",
    "    tools=[upsert_memory],\n",
    "    prompt=prepare_messages,\n",
    "    store=store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since you prefer Italian food, how about we find some Italian restaurants nearby? Would you like me to help with that? If so, I'll need your location to proceed. Alternatively, if you're looking for a quick meal, there might be some Italian dishes that are easy to make at home. Let me know what you'd prefer!"
     ]
    }
   ],
   "source": [
    "for msg, metadata in agent.stream(\n",
    "    input={\"messages\" : [{\"role\": \"user\", \"content\" : \"I'm hungry\"}]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(msg.content, end =\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-vector indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect mem2\n",
      "Item: mem2; Score (0.7748096097731049)\n",
      "Memory: Ate alone at home\n",
      "Emotion: felt a bit lonely\n",
      "\n",
      "Expect mem1\n",
      "Item: mem1; Score (0.6843336968592125)\n",
      "Memory: Had pizza with friends at Mario's\n",
      "Emotion: felt happy and connected\n",
      "\n",
      "Expect random lower score (ravioli not indexed)\n",
      "Item: mem1; Score (0.536263103505899)\n",
      "Memory: Had pizza with friends at Mario's\n",
      "Emotion: felt happy and connected\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "embeddings = init_embeddings('huggingface:BAAI/bge-m3')\n",
    "\n",
    "# Configure store to embed both memory content and emotional context\n",
    "store = InMemoryStore(\n",
    "    index = {'embed' : embeddings , 'dims': 1024, \"fields\": [\"memory\", \"emotional_context\"]}\n",
    ")\n",
    "\n",
    "# Store memories with different content/emotion pairs\n",
    "store.put(\n",
    "    (\"user_123\", \"memories\"),\n",
    "    \"mem1\",\n",
    "    {\n",
    "        \"memory\" : \"Had pizza with friends at Mario's\",\n",
    "        \"emotional_context\" : \"felt happy and connected\",\n",
    "        \"this_isnt_idexed\" : \"I perfer ravioli though\"\n",
    "    }\n",
    ")\n",
    "\n",
    "store.put(\n",
    "    (\"user_123\", \"memories\"),\n",
    "    \"mem2\",\n",
    "    {\n",
    "        \"memory\": \"Ate alone at home\",\n",
    "        \"emotional_context\": \"felt a bit lonely\",\n",
    "        \"this_isnt_indexed\": \"I like pie\",\n",
    "    },\n",
    ")\n",
    "\n",
    "results = store.search(\n",
    "    (\"user_123\",\"memories\"), query=\"times they felt isolated\", limit=1\n",
    ")\n",
    "\n",
    "print(\"Expect mem2\")\n",
    "for r in results:\n",
    "    print(f\"Item: {r.key}; Score ({r.score})\")\n",
    "    print(f\"Memory: {r.value['memory']}\")\n",
    "    print(f\"Emotion: {r.value['emotional_context']}\\n\")\n",
    "\n",
    "print(\"Expect mem1\")\n",
    "results = store.search((\"user_123\", \"memories\"), query=\"fun pizza\", limit=1)\n",
    "for r in results:\n",
    "    print(f\"Item: {r.key}; Score ({r.score})\")\n",
    "    print(f\"Memory: {r.value['memory']}\")\n",
    "    print(f\"Emotion: {r.value['emotional_context']}\\n\")\n",
    "\n",
    "print(\"Expect random lower score (ravioli not indexed)\")\n",
    "results = store.search((\"user_123\", \"memories\"), query=\"ravioli\", limit=1)\n",
    "for r in results:\n",
    "    print(f\"Item: {r.key}; Score ({r.score})\")\n",
    "    print(f\"Memory: {r.value['memory']}\")\n",
    "    print(f\"Emotion: {r.value['emotional_context']}\\n\") ## fields 에 포함시켜준 애만 Indexing이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Override fields at storage time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect mem1\n",
      "Item: mem2; Score (0.6082269820388908)\n",
      "Memory: The restaurant was too loud\n",
      "Context: Dinner at an Italian place\n",
      "\n",
      "Expect mem2\n",
      "Item: mem2; Score (0.6742498352381826)\n",
      "Memory: The restaurant was too loud\n",
      "Context: Dinner at an Italian place\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store = InMemoryStore(\n",
    "    index=(\n",
    "        {\n",
    "            'embed': embeddings,\n",
    "            'dims' : 1024,\n",
    "            'fileds' : ['memory'],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Store one memory with default indexing\n",
    "store.put(\n",
    "    (\"user_123\", \"memories\"),\n",
    "    \"mem1\",\n",
    "    {\"memory\": \"I love spicy food\", \"context\": \"At a Thai restaurant\"},\n",
    ")\n",
    "\n",
    "# Store another overriding which fields to embed\n",
    "store.put(\n",
    "    (\"user_123\", \"memories\"),\n",
    "    \"mem2\",\n",
    "    {\"memory\": \"The restaurant was too loud\", \"context\": \"Dinner at an Italian place\"},\n",
    "    index=[\"context\"],  # Override: only embed the context\n",
    ")\n",
    "\n",
    "# Search about food - matches mem1 (using default field)\n",
    "print(\"Expect mem1\")\n",
    "results = store.search(\n",
    "    (\"user_123\", \"memories\"), query=\"what food do they like\", limit=1\n",
    ")\n",
    "for r in results:\n",
    "    print(f\"Item: {r.key}; Score ({r.score})\")\n",
    "    print(f\"Memory: {r.value['memory']}\")\n",
    "    print(f\"Context: {r.value['context']}\\n\")\n",
    "\n",
    "# Search about restaurant atmosphere - matches mem2 (using overridden field)\n",
    "print(\"Expect mem2\")\n",
    "results = store.search(\n",
    "    (\"user_123\", \"memories\"), query=\"restaurant environment\", limit=1\n",
    ")\n",
    "for r in results:\n",
    "    print(f\"Item: {r.key}; Score ({r.score})\")\n",
    "    print(f\"Memory: {r.value['memory']}\")\n",
    "    print(f\"Context: {r.value['context']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable Indexing for Specific Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect mem1\n",
      "Item: mem1; Score (0.5142238669564672)\n",
      "Memory: I love chocolate ice cream\n",
      "Type: preference\n",
      "\n",
      "Expect low score (mem2 not indexed)\n",
      "Item: mem1; Score (0.4149083716517455)\n",
      "Memory: I love chocolate ice cream\n",
      "Type: preference\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store = InMemoryStore(index={\"embed\": embeddings, \"dims\": 1536, \"fields\": [\"memory\"]})\n",
    "\n",
    "# Store a normal indexed memory\n",
    "store.put(\n",
    "    (\"user_123\", \"memories\"),\n",
    "    \"mem1\",\n",
    "    {\"memory\": \"I love chocolate ice cream\", \"type\": \"preference\"},\n",
    ")\n",
    "\n",
    "# Store a system memory without indexing\n",
    "store.put(\n",
    "    (\"user_123\", \"memories\"),\n",
    "    \"mem2\",\n",
    "    {\"memory\": \"User completed onboarding\", \"type\": \"system\"},\n",
    "    index=False,  # Disable indexing entirely\n",
    ")\n",
    "\n",
    "# Search about food preferences - finds mem1\n",
    "print(\"Expect mem1\")\n",
    "results = store.search((\"user_123\", \"memories\"), query=\"what food preferences\", limit=1)\n",
    "for r in results:\n",
    "    print(f\"Item: {r.key}; Score ({r.score})\")\n",
    "    print(f\"Memory: {r.value['memory']}\")\n",
    "    print(f\"Type: {r.value['type']}\\n\")\n",
    "\n",
    "# Search about onboarding - won't find mem2 (not indexed)\n",
    "print(\"Expect low score (mem2 not indexed)\")\n",
    "results = store.search((\"user_123\", \"memories\"), query=\"onboarding status\", limit=1)\n",
    "for r in results:\n",
    "    print(f\"Item: {r.key}; Score ({r.score})\")\n",
    "    print(f\"Memory: {r.value['memory']}\")\n",
    "    print(f\"Type: {r.value['type']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-agent-x8cjzk9S-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
